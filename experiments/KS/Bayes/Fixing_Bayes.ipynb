{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we fix the Bayes estimator, as it's not working. As a baseline, we take the sk-learn algo. Those are the values we need to hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# DeepMoD stuff\n",
    "from multitaskpinn import DeepMoD\n",
    "from multitaskpinn.model.func_approx import Siren, NN\n",
    "from multitaskpinn.model.library import Library1D\n",
    "from multitaskpinn.model.constraint import LeastSquares\n",
    "from multitaskpinn.model.sparse_estimators import Threshold \n",
    "\n",
    "from phimal_utilities.data import Dataset\n",
    "from phimal_utilities.data.burgers import BurgersDelta\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device ='cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# Settings for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=1000, noise=0.2, random=True, normalize=False)\n",
    "X, y = X.to(device), y.to(device)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) # Sparse estimator \n",
    "constraint = LeastSquares() # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data out\n",
    "prediction, time_derivs, thetas = model(X)\n",
    "t = time_derivs[0].cpu().detach().numpy()\n",
    "theta = thetas[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=0, alpha_2=0, compute_score=True, fit_intercept=False,\n",
       "              lambda_1=0, lambda_2=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting sklearn results\n",
    "sk_reg = BayesianRidge(fit_intercept=False, compute_score=True, alpha_1=0, alpha_2=0, lambda_1=0, lambda_2=0)\n",
    "sk_reg.fit(theta, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624313.8692322593 1.601758425180807e-06\n"
     ]
    }
   ],
   "source": [
    "# Precision, noise level\n",
    "print(sk_reg.alpha_, 1 / sk_reg.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014547715291435611 68.73931610338224\n"
     ]
    }
   ],
   "source": [
    "# Precision of prior, std of prior\n",
    "print(sk_reg.lambda_, 1 / sk_reg.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.48640479e-02  3.63397636e+00 -1.24136497e+00 -8.73036119e-02\n",
      " -3.47944061e-01  2.09853651e+01 -2.43294908e+00 -9.55226105e-01\n",
      " -1.40345956e+00 -4.65038034e+00  1.18284549e+01 -1.38515284e+00]\n"
     ]
    }
   ],
   "source": [
    "# Found coeffs\n",
    "print(sk_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5871360089037915e-06"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What would the reg cost be; probably similar to the noise level\n",
    "np.mean((t - theta @ sk_reg.coef_[:, None])**2) # beware; sk learn coeff output is 1d and will give wrong results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5201.086546976798"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the score aka the log evidence\n",
    "sk_reg.scores_[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which sounds reasonable but it isn't at all; now let's implement our own and see if we can get it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data out\n",
    "prediction, time_derivs, thetas = model(X)\n",
    "t = time_derivs[0]\n",
    "Theta = thetas[0]\n",
    "\n",
    "# Let's use the found alpha and beta to check if the code is good; later we see if we can find it by optimizing\n",
    "alpha = torch.tensor(sk_reg.lambda_) # precision of weights; follow sk learn init\n",
    "beta = torch.tensor(sk_reg.alpha_) # precision of noise\n",
    "\n",
    "M = Theta.shape[1]\n",
    "N = Theta.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior std and mean\n",
    "A = torch.eye(M).to(Theta.device) * alpha + beta * Theta.T @ Theta  \n",
    "mn = beta * torch.inverse(A) @ Theta.T @ t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00117785],\n",
       "       [0.01039181],\n",
       "       [0.00786998],\n",
       "       [0.00532904],\n",
       "       [0.0152518 ],\n",
       "       [0.13535671],\n",
       "       [0.07296802],\n",
       "       [0.05193206],\n",
       "       [0.04800464],\n",
       "       [0.29380104],\n",
       "       [0.07156858],\n",
       "       [0.15302918]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference between posterior means; seems within numerical acc.\n",
    "np.abs(mn.detach().cpu().numpy() - sk_reg.coef_[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5dec1bedb306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Difference between posterior std; seems within numerical acc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msk_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mSn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Sn' is not defined"
     ]
    }
   ],
   "source": [
    "# Difference between posterior std; seems within numerical acc.\n",
    "np.mean(np.abs(sk_reg.sigma_ - Sn.detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5172.8711]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/2 * (M * torch.log(alpha) \n",
    " + N * torch.log(beta)\n",
    " - beta * (t - Theta @ mn).T @ (t - Theta @ mn) - alpha * mn.T @ mn \n",
    " - torch.trace(torch.log(A))\n",
    " - N * np.log(2*np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is close enough given numerical accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_marginal_LL(Theta, t, alpha, beta):\n",
    "    M = Theta.shape[1]\n",
    "    N = Theta.shape[0]\n",
    "    \n",
    "    \n",
    "    # Posterior std and mean\n",
    "    A = torch.eye(M).to(Theta.device) * alpha + beta * Theta.T @ Theta  \n",
    "    mn = beta * torch.inverse(A) @ Theta.T @ t\n",
    "    \n",
    "    loss = -1/2 * (M * torch.log(alpha) \n",
    "         + N * torch.log(beta)\n",
    "         - beta * (t - Theta @ mn).T @ (t - Theta @ mn) - alpha * mn.T @ mn \n",
    "         - torch.trace(torch.log(A))\n",
    "         - N * np.log(2*np.pi))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5172.6836]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_marginal_LL(Theta, t, torch.tensor(sk_reg.lambda_), torch.tensor(sk_reg.alpha_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(0., requires_grad=True) Parameter containing:\n",
      "tensor(11.7559, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Now let's try to optimize using gradient descent\n",
    "prediction, time_derivs, thetas = model(X)\n",
    "t = time_derivs[0].detach().cpu()\n",
    "Theta = thetas[0].detach().cpu()\n",
    "\n",
    "alpha = torch.nn.Parameter(torch.log(torch.tensor(1.0))) # precision of weights; follow sk learn init\n",
    "beta = torch.nn.Parameter(torch.log(torch.tensor(1 / torch.var(t)))) # precision of noise\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': alpha}, {'params': beta}])\n",
    "\n",
    "print(alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5163.2783]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5163.6104]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5161.1948]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5158.9771]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5158.8398]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5158.6357]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5158.9561]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5158.9346]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5158.9199]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5157.8462]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5158.8833]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5158.4561]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5155.8662]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5155.8442]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5155.7070]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5156.1035]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5155.9780]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5152.3306]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5152.6489]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5133.8140]], grad_fn=<MulBackward0>)\n",
      "tensor([[-4934.7769]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5148.8301]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5143.8105]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5143.9160]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5144.0884]], grad_fn=<MulBackward0>)\n",
      "tensor([[-5144.1025]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ce3c11ecaea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in torch.arange(100000):\n",
    "    loss = neg_marginal_LL(Theta, t, torch.exp(alpha), torch.exp(beta))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0929, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(810770.6875, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014547715291435611 624313.8692322593\n"
     ]
    }
   ],
   "source": [
    "print(sk_reg.lambda_, sk_reg.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
