{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work on the KS? Let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# DeepMoD stuff\n",
    "from deepymod import DeepMoD\n",
    "from deepymod.model.func_approx import NN, Siren\n",
    "from deepymod.model.library import Library1D\n",
    "from deepymod.model.constraint import LeastSquares\n",
    "from deepymod.model.sparse_estimators import Threshold\n",
    "from deepymod.training.sparsity_scheduler import TrainTestPeriodic, Periodic, TrainTest\n",
    "\n",
    "from deepymod.data import Dataset\n",
    "from deepymod.data.burgers import BurgersDelta\n",
    "\n",
    "from deepymod.utils.logger import Logger\n",
    "from deepymod.training.convergence import Convergence\n",
    "from scipy.io import loadmat\n",
    "from deepymod.analysis import load_tensorboard\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "# Settings for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping data\n",
    "data = loadmat('kuramoto_sivishinky.mat')\n",
    "\n",
    "t = data['tt']\n",
    "x = data['x']\n",
    "u = data['uu']\n",
    "\n",
    "# Normalizing data\n",
    "t = (t - t.min())/(t.max()-t.min()) * 2 - 1\n",
    "x = (x - x.min())/(x.max()-x.min()) * 2 - 1\n",
    "\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "\n",
    "# Limiting to non-chaotic part\n",
    "lower_lim = 80\n",
    "x_grid = x_grid[:, lower_lim:]\n",
    "t_grid = t_grid[:, lower_lim:]\n",
    "u = u[:, lower_lim:]\n",
    "\n",
    "# %%Making training data\n",
    "X = np.concatenate((t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)), axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "\n",
    "# Adding noise\n",
    "noise_level = 0.05\n",
    "y_noisy = y + noise_level * np.std(y, axis=0) * np.random.randn(*y.shape)\n",
    "number_of_samples = 25000\n",
    "\n",
    "# Into tensor\n",
    "idx = np.random.permutation(y.shape[0])\n",
    "X_train = torch.tensor(X[idx, :][:number_of_samples], dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_noisy[idx, :][:number_of_samples], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SBL(model: DeepMoD,\n",
    "          data: torch.Tensor,\n",
    "          target: torch.Tensor,\n",
    "          optimizer,\n",
    "          extra_params, \n",
    "          sparsity_scheduler,\n",
    "          split = 0.8,\n",
    "          exp_ID: str = None,\n",
    "          log_dir: str = None,\n",
    "          max_iterations: int = 10000,\n",
    "          write_iterations: int = 25,\n",
    "          **convergence_kwargs) -> None:\n",
    "    \"\"\"Trains the DeepMoD model. This function automatically splits the data set in a train and test set. \n",
    "\n",
    "    Args:\n",
    "        model (DeepMoD):  A DeepMoD object.\n",
    "        data (torch.Tensor):  Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate.\n",
    "        target (torch.Tensor): Tensor of shape (n_samples x n_features) containing the target data.\n",
    "        optimizer ([type]):  Pytorch optimizer.\n",
    "        sparsity_scheduler ([type]):  Decides when to update the sparsity mask.\n",
    "        split (float, optional):  Fraction of the train set, by default 0.8.\n",
    "        exp_ID (str, optional): Unique ID to identify tensorboard file. Not used if log_dir is given, see pytorch documentation.\n",
    "        log_dir (str, optional): Directory where tensorboard file is written, by default None.\n",
    "        max_iterations (int, optional): [description]. Max number of epochs , by default 10000.\n",
    "        write_iterations (int, optional): [description]. Sets how often data is written to tensorboard and checks train loss , by default 25.\n",
    "    \"\"\"\n",
    "    logger = Logger(exp_ID, log_dir)\n",
    "    sparsity_scheduler.path = logger.log_dir # write checkpoint to same folder as tb output.\n",
    "    \n",
    "    t, a, l = extra_params\n",
    "    \n",
    "    # Splitting data, assumes data is already randomized\n",
    "    n_train = int(split * data.shape[0])\n",
    "    n_test = data.shape[0] - n_train\n",
    "    data_train, data_test = torch.split(data, [n_train, n_test], dim=0)\n",
    "    target_train, target_test = torch.split(target, [n_train, n_test], dim=0)\n",
    "    \n",
    "    M = 10\n",
    "    N = data_train.shape[0]\n",
    "    threshold = torch.tensor(1e4).to(data.device)\n",
    "    \n",
    "    alpha_threshold = torch.tensor(1e8).to(data.device)\n",
    "    \n",
    "    # Training\n",
    "    convergence = Convergence(**convergence_kwargs)\n",
    "    for iteration in torch.arange(0, max_iterations):\n",
    "        # ================== Training Model ============================\n",
    "        prediction, time_derivs, thetas = model(data_train)\n",
    "        \n",
    "        tau_ = torch.exp(t)\n",
    "        alpha_ = torch.min(torch.exp(a), alpha_threshold)\n",
    "        lambda_ = torch.min(torch.exp(l), 2 * threshold)\n",
    "                            \n",
    "        y = time_derivs[0]\n",
    "        X = thetas[0] / torch.norm(thetas[0], dim=0, keepdim=True)\n",
    "        \n",
    "        p_MSE = N / 2 * (tau_ * torch.mean((prediction - target_train)**2, dim=0) - t + np.log(2*np.pi))\n",
    "        \n",
    "        A = torch.diag(lambda_) + alpha_ * X.T @ X\n",
    "        mn = (lambda_ < threshold)[:, None] * (alpha_ * torch.inverse(A) @ X.T @ y)\n",
    "        E = alpha_ * torch.sum((y - X @ mn)**2) + mn.T @ torch.diag(lambda_) @ mn\n",
    "        p_reg = 1/2 * (E + torch.sum(torch.log(torch.diag(A)[lambda_ < threshold])) - (torch.sum(l[lambda_ < threshold]) + N * a) - N * np.log(2*np.pi))\n",
    "\n",
    "        MSE = torch.mean((prediction - target_train)**2, dim=0)  # loss per output\n",
    "        Reg = torch.stack([torch.mean((dt - theta @ coeff_vector)**2)\n",
    "                           for dt, theta, coeff_vector in zip(time_derivs, thetas, model.constraint_coeffs(scaled=False, sparse=True))])\n",
    "        loss = torch.sum(p_MSE + p_reg)\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration % write_iterations == 0:\n",
    "            # ================== Validation costs ================\n",
    "            with torch.no_grad():\n",
    "                prediction_test = model.func_approx(data_test)[0]\n",
    "                MSE_test = torch.mean((prediction_test - target_test)**2, dim=0)  # loss per output\n",
    "         \n",
    "            # ====================== Logging =======================\n",
    "            _ = model.sparse_estimator(thetas, time_derivs) # calculating estimator coeffs but not setting mask\n",
    "            logger(iteration, \n",
    "                   loss, MSE, Reg,\n",
    "                   model.constraint_coeffs(sparse=True, scaled=True), \n",
    "                   model.constraint_coeffs(sparse=True, scaled=False),\n",
    "                   model.estimator_coeffs(),\n",
    "                   MSE_test=MSE_test,\n",
    "                   p_MSE = p_MSE,\n",
    "                   p_reg = p_reg,\n",
    "                   tau = tau_,\n",
    "                   alpha=alpha_,\n",
    "                  lambda_=lambda_,\n",
    "                   mn=mn)\n",
    "\n",
    "            # ================== Sparsity update =============\n",
    "            # Updating sparsity \n",
    "            update_sparsity = sparsity_scheduler(iteration, torch.sum(MSE_test), model, optimizer)\n",
    "            if update_sparsity: \n",
    "                model.constraint.sparsity_masks = model.sparse_estimator(thetas, time_derivs)\n",
    "\n",
    "            # ================= Checking convergence\n",
    "            l1_norm = torch.sum(torch.abs(torch.cat(model.constraint_coeffs(sparse=True, scaled=True), dim=1)))\n",
    "            converged = convergence(iteration, l1_norm)\n",
    "            if converged:\n",
    "                break\n",
    "    logger.close(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.nn.Parameter(-torch.log(torch.var(y_train)).to(device))\n",
    "a = torch.nn.Parameter(-torch.log(torch.var(y_train)).to(device))\n",
    "l  = torch.nn.Parameter(torch.zeros(10).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%Configuring model\n",
    "network = Siren(2, [50, 50, 50, 50, 50, 50, 50, 50], 1) # Function approximator\n",
    "library =  Library1D(poly_order=1, diff_order=4) # Library function\n",
    "estimator = Threshold(0.1)#PDEFIND(lam=1e-6, dtol=0.1) # Sparse estimator \n",
    "constraint = LeastSquares() # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model\n",
    "\n",
    "# %% Setting schedulers\n",
    "sparsity_scheduler = TrainTestPeriodic(patience=8, delta=1e-9)#Periodic(initial_epoch=10000, periodicity=100) # Defining when to apply sparsity\n",
    "optimizer = torch.optim.Adam([{'params':model.parameters(), 'betas':(0.999, 0.999), 'amsgrad':True, 'lr':0.00025}, {'params':[t, a, l], 'betas':(0.99, 0.999), 'amsgrad':True, 'lr':2e-3}]) # Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26050  MSE: 1.45e+00  Reg: 2.65e-15  L1: 0.00e+00 "
     ]
    }
   ],
   "source": [
    "train_SBL(model, X_train, y_train, optimizer, [t, a, l], sparsity_scheduler, exp_ID='KS_fast_params', split=0.8, write_iterations=50, max_iterations=50000, delta=0.0, patience=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
