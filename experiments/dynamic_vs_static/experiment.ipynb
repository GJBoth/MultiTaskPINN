{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we test the difference between dynamic using multitask learning and static using the results from the multitask learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# DeepMoD stuff\n",
    "from multitaskpinn import DeepMoD\n",
    "from multitaskpinn.model.func_approx import Siren, NN\n",
    "from multitaskpinn.model.library import Library1D\n",
    "from multitaskpinn.model.constraint import LeastSquares\n",
    "from multitaskpinn.model.sparse_estimators import Threshold\n",
    "from multitaskpinn.training import train, train_multitask\n",
    "from multitaskpinn.training.sparsity_scheduler import TrainTestPeriodic, s_periodic, Periodic\n",
    "\n",
    "from phimal_utilities.data import Dataset\n",
    "from phimal_utilities.data.burgers import BurgersDelta\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device ='cuda'\n",
    "#else:\n",
    "#    device = 'cpu'\n",
    "    \n",
    "device = 'cpu'\n",
    "\n",
    "# Settings for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset\n",
    "v = 0.1\n",
    "A = 1.0\n",
    "\n",
    "x = np.linspace(-3, 4, 100)\n",
    "t = np.linspace(0.5, 5.0, 50)\n",
    "x_grid, t_grid = np.meshgrid(x, t, indexing='ij')\n",
    "dataset = Dataset(BurgersDelta, v=v, A=A)\n",
    "\n",
    "X, y = dataset.create_dataset(x_grid.reshape(-1, 1), t_grid.reshape(-1, 1), n_samples=1000, noise=0.1, random=True, normalize=True)\n",
    "X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) # Sparse estimator \n",
    "constraint = LeastSquares() # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "      10000    100.00%               0s   -1.70e+01   3.47e-04   1.04e-07   3.06e+00 "
     ]
    }
   ],
   "source": [
    "sparsity_scheduler = Periodic(initial_epoch=20000)#s_periodic(patience=100, delta=0.05, periodicity=100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer\n",
    "    \n",
    "train_multitask(model, X, y, optimizer, sparsity_scheduler, write_iterations=25, log_dir='runs/multitask_dynamic/', max_iterations=10000, delta=0.00, patience=100) # Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DeepMoD' object has no attribute 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-68b1a86f7c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    594\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DeepMoD' object has no attribute 's'"
     ]
    }
   ],
   "source": [
    "model.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.9959, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(-model.s)[0, 1] / torch.exp(-model.s)[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 300 times as big..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from math import pi\n",
    "import numpy as np\n",
    "\n",
    "from multitaskpinn.utils.tensorboard import Tensorboard\n",
    "from multitaskpinn.utils.output import progress\n",
    "from multitaskpinn.training.convergence import Convergence\n",
    "from multitaskpinn.model.deepmod import DeepMoD\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_static(model: DeepMoD,\n",
    "          data: torch.Tensor,\n",
    "          target: torch.Tensor,\n",
    "          loss_weight,\n",
    "          optimizer,\n",
    "          sparsity_scheduler,\n",
    "          log_dir: Optional[str] = None,\n",
    "          max_iterations: int = 10000,\n",
    "          write_iterations: int = 25,\n",
    "          **convergence_kwargs) -> None:\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        model (DeepMoD): [description]\n",
    "        data (torch.Tensor): [description]\n",
    "        target (torch.Tensor): [description]\n",
    "        optimizer ([type]): [description]\n",
    "        sparsity_scheduler ([type]): [description]\n",
    "        log_dir (Optional[str], optional): [description]. Defaults to None.\n",
    "        max_iterations (int, optional): [description]. Defaults to 10000.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    board = Tensorboard(log_dir)  # initializing tb board\n",
    "\n",
    "    # Training\n",
    "    convergence = Convergence(**convergence_kwargs)\n",
    "    print('| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |')\n",
    "    for iteration in np.arange(0, max_iterations + 1):\n",
    "        # ================== Training Model ============================\n",
    "        prediction, time_derivs, thetas = model(data)\n",
    "\n",
    "        MSE = torch.mean((prediction - target)**2, dim=0)  # loss per output\n",
    "        Reg = torch.stack([torch.mean((dt - theta @ coeff_vector)**2)\n",
    "                           for dt, theta, coeff_vector in zip(time_derivs, thetas, model.constraint_coeffs(scaled=False, sparse=True))])\n",
    "        loss = torch.sum(MSE + loss_weight * Reg)  # 1e-5 for numerical stability\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ====================== Logging =======================\n",
    "        # We calculate the normalization factor and the l1_norm\n",
    "        l1_norm = torch.sum(torch.abs(torch.cat(model.constraint_coeffs(sparse=True, scaled=True), dim=1)), dim=0)\n",
    "\n",
    "        # Write progress to command line and tensorboard\n",
    "        if iteration % write_iterations == 0:\n",
    "            _ = model.sparse_estimator(thetas, time_derivs) # calculating l1 adjusted coeffs but not setting mask\n",
    "            progress(iteration, start_time, max_iterations, loss.item(),\n",
    "                     torch.sum(MSE).item(), torch.sum(Reg).item(), torch.sum(l1_norm).item())\n",
    "\n",
    "            if model.estimator_coeffs() is None:\n",
    "                estimator_coeff_vectors = [torch.zeros_like(coeff) for coeff in model.constraint_coeffs(sparse=True, scaled=False)] # It doesnt exist before we start sparsity, so we use zeros\n",
    "            else:\n",
    "                estimator_coeff_vectors = model.estimator_coeffs()\n",
    "\n",
    "            board.write(iteration, loss, MSE, Reg, l1_norm, model.constraint_coeffs(sparse=True, scaled=True), model.constraint_coeffs(sparse=True, scaled=False), estimator_coeff_vectors)\n",
    "            \n",
    "        # ================== Validation and sparsity =============\n",
    "        # Updating sparsity and or convergence\n",
    "        sparsity_scheduler(iteration, torch.sum(l1_norm))\n",
    "        if sparsity_scheduler.apply_sparsity is True:\n",
    "            with torch.no_grad():\n",
    "                model.constraint.sparsity_masks = model.sparse_estimator(thetas, time_derivs)\n",
    "                sparsity_scheduler.reset()\n",
    "                print(model.sparsity_masks)\n",
    "\n",
    "        # Checking convergence\n",
    "        convergence(iteration, torch.sum(l1_norm))\n",
    "        if convergence.converged is True:\n",
    "            print('Algorithm converged. Stopping training.')\n",
    "            break\n",
    "    board.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30, 30], 1)\n",
    "library = Library1D(poly_order=2, diff_order=3) # Library function\n",
    "estimator = Threshold(0.1) # Sparse estimator \n",
    "constraint = LeastSquares() # How to constrain\n",
    "model = DeepMoD(network, library, estimator, constraint).to(device) # Putting it all in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Iteration | Progress | Time remaining |     Loss |      MSE |      Reg |    L1 norm |\n",
      "       2550     25.50%             412s   1.27e-02   1.25e-02   3.64e-07   3.91e+00 [tensor([False,  True,  True,  True, False, False,  True, False,  True, False,\n",
      "        False, False])]\n",
      "       2650     26.50%             405s   1.88e-02   1.81e-02   2.26e-06   1.36e+00 [tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False])]\n",
      "       2750     27.50%             397s   1.80e-02   1.73e-02   2.65e-06   1.00e+00 [tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False])]\n",
      "Algorithm converged. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "sparsity_scheduler = s_periodic(patience=100, delta=0.05, periodicity=100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), amsgrad=True) # Defining optimizer\n",
    "    \n",
    "train_static(model, X, y, 300, optimizer, sparsity_scheduler, write_iterations=25, log_dir='runs/multitask_static/', max_iterations=10000, delta=0.01, patience=100) # Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
